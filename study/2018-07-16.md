date: 2018-07-16 22:09:36
layout: post
tags: ['python', 'pandas', 'report']
title: pandas large data to_csv bug

[#21643](https://github.com/pandas-dev/pandas/issues/21643)

issue 는 closed 상태이지만 아직 현재 버전에서 해결 되지 않았습니다.

대용량의 `DataFrame` 을 생성한 후 `to_csv` 가 동작하지 않는 현상입니다.
이 때 제가 다루었던 DataFrame 이 300000 x 1100 이었습니다.
위의 이슈는 직접 해보니 5000000 x 65 입니다. 데이터프레임 크기는 2GB 넘어서 비슷했습니다.

`read_csv` 에러는 많이 봤어도 이 에러는 처음 접하는 듯 합니다.

이 에러는 데이터를 쪼개서 저장하는 것으로 임시로 해결하였습니다.
하지만 `chunksize` 옵션으로 해결할 순 없었습니다.

`np.linspace` 함수로 데이터를 적절하게 쪼개서 차곡차곡 append 시켰습니다.
처음에만 헤더를 출력하는 로직은 이터레이터를 사용하였습니다.

```python
import numpy as np
import pandas as pd


def df_chunks(df, filename, num=50):
    ar = np.linspace(0, df.index.size, dtype=int, num=num)
    it = zip(ar, ar[1:])

    sl1 = next(it)
    df.loc[slice(*sl1)].to_csv(filename, index=False)

    for sl in it:
        start, stop = sl
        sl = slice(start + 1, stop)
        df.loc[sl].to_csv(filename, mode='a', index=False, header=False)
```

이 코드로 위의 이슈도 잠시나마 해결할 수는 있었습니다.
다시 `read_csv` 로 되돌리기도 가능했습니다.
이슈 진행상황을 현재로써 보기에는 다음 버전에 해결될 것으로 보입니다.

언젠가 저도 크게 기여했으면 하는 바람입니다.

